{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input\n",
    "tweet = u'#HaceInstantes Abrazo al Congreso organizado por gremios docentes después de la represión policial de ayer. https://t.co/ToyertpI70'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#HaceInstantes Abrazo al Congreso organizado por gremios docentes después de la represión policial de ayer. https://t.co/ToyertpI70\n"
     ]
    }
   ],
   "source": [
    "print tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import operator\n",
    "import re\n",
    "import gc\n",
    "import gensim\n",
    "\n",
    "from tw_dataset.dbmodels import *\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(doc):\n",
    "    pre_doc = doc\n",
    "        \n",
    "    # remover URLs\n",
    "    pre_doc = re.sub(\n",
    "        r\"https?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\",\n",
    "        \" \", pre_doc)\n",
    "    \n",
    "    # minúsculas\n",
    "    pre_doc = pre_doc.lower()\n",
    "\n",
    "    # volar acentos\n",
    "    pre_doc = gensim.utils.deaccent(pre_doc)\n",
    "\n",
    "    # remove bullshit\n",
    "    pre_doc = re.sub(r\"\\@|\\'|\\\"|\\\\|…|\\/|\\-|\\||\\(|\\)|\\.|\\,|\\!|\\?|\\:|\\;|“|”|’|—\", \" \", pre_doc)\n",
    "    \n",
    "    # contraer vocales\n",
    "    for v in 'aeiou':\n",
    "        pre_doc = re.sub(r\"[%s]+\" % v, v, pre_doc)    \n",
    "    \n",
    "    # normalizar espacio en blanco\n",
    "    pre_doc = re.sub(r\"\\s+\", \" \", pre_doc)\n",
    "    pre_doc = re.sub(r\"(^\\s)|(\\s$)\", \"\", pre_doc)\n",
    "    \n",
    "    return pre_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.data import load\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "\n",
    "spanish_tokenizer = load('tokenizers/punkt/spanish.pickle')\n",
    "\n",
    "# stopwords en español\n",
    "spanish_stopwords = stopwords.words('spanish')\n",
    "\n",
    "# spanish stemmer\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "# punctuation to remove\n",
    "non_words = list(punctuation)\n",
    "\n",
    "# we add spanish punctuation\n",
    "non_words.extend(['¿', '¡'])\n",
    "non_words.extend(map(str, range(10)))\n",
    "\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "\n",
    "def tokenize(text, stem=True, remove_stopwords=False):\n",
    "    text = text.lower()\n",
    "    result = []\n",
    "    \n",
    "    for sentence in spanish_tokenizer.tokenize(text):\n",
    "        # remover puntuación\n",
    "        text = ''.join([c for c in sentence if c not in non_words])\n",
    "        \n",
    "        # tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        if remove_stopwords:\n",
    "            tokens = [t for t in tokens if t not in spanish_stopwords]\n",
    "\n",
    "        # stem\n",
    "        if stem:\n",
    "            tokens = [stemmer.stem(t) for t in tokens]\n",
    "        \n",
    "        # tokens de al menos 2 letras\n",
    "        tokens = [t for t in tokens if len(t) > 1]\n",
    "        \n",
    "        result += tokens\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#HaceInstantes Abrazo al Congreso organizado por gremios docentes después de la represión policial de ayer. https://t.co/ToyertpI70\n"
     ]
    }
   ],
   "source": [
    "print tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#haceinstantes abrazo al congreso organizado por gremios docentes despues de la represion policial de ayer\n"
     ]
    }
   ],
   "source": [
    "tweet = normalize(tweet)\n",
    "print tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'haceinst', u'abraz', u'al', u'congres', u'organiz', u'por', u'gremi', u'docent', u'despu', u'de', u'la', u'represion', u'policial', u'de', u'ayer']\n"
     ]
    }
   ],
   "source": [
    "tweet = tokenize(tweet)\n",
    "print tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class get_docs(object):\n",
    "    def __init__(self, corpus):\n",
    "        self.corpus = corpus\n",
    "\n",
    "    def __iter__(self):\n",
    "        for doc in self.corpus:\n",
    "            tokens = tokenize(preprocess(doc), remove_stopwords=True)\n",
    "            yield tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phraser, Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phrases = Phrases(get_docs(corpus), min_count=5)\n",
    "bigram = Phraser(phrases)\n",
    "trigram = Phrases(bigram[get_docs(corpus)], min_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-29 20:49:40,029 : INFO : collecting all words and their counts\n",
      "2017-04-29 20:49:40,043 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2017-04-29 20:50:20,050 : INFO : PROGRESS: at sentence #10000, processed 72026 words and 74599 word types\n",
      "2017-04-29 20:51:00,038 : INFO : PROGRESS: at sentence #20000, processed 143398 words and 134683 word types\n",
      "2017-04-29 20:51:45,573 : INFO : PROGRESS: at sentence #30000, processed 212618 words and 188898 word types\n",
      "2017-04-29 20:52:26,366 : INFO : PROGRESS: at sentence #40000, processed 284308 words and 243904 word types\n",
      "2017-04-29 20:53:08,527 : INFO : PROGRESS: at sentence #50000, processed 355860 words and 296243 word types\n",
      "2017-04-29 20:53:49,600 : INFO : PROGRESS: at sentence #60000, processed 427084 words and 346451 word types\n",
      "2017-04-29 20:54:30,899 : INFO : PROGRESS: at sentence #70000, processed 500042 words and 396439 word types\n",
      "2017-04-29 20:55:15,741 : INFO : PROGRESS: at sentence #80000, processed 571445 words and 443676 word types\n",
      "2017-04-29 20:55:54,436 : INFO : PROGRESS: at sentence #90000, processed 643360 words and 489345 word types\n",
      "2017-04-29 20:56:37,135 : INFO : PROGRESS: at sentence #100000, processed 715738 words and 535067 word types\n",
      "2017-04-29 20:57:15,119 : INFO : collected 575852 word types from a corpus of 779331 words (unigram + bigrams) and 109040 sentences\n",
      "2017-04-29 20:57:15,120 : INFO : using 575852 counts as vocab in Phrases<0 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000>\n",
      "2017-04-29 20:57:15,176 : INFO : source_vocab length 575852\n",
      "2017-04-29 20:57:21,509 : INFO : Phraser built with 6054 6054 phrasegrams\n",
      "2017-04-29 20:57:21,513 : INFO : collecting all words and their counts\n",
      "2017-04-29 20:57:21,517 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2017-04-29 20:58:04,638 : INFO : PROGRESS: at sentence #10000, processed 65211 words and 75358 word types\n",
      "2017-04-29 20:58:46,755 : INFO : PROGRESS: at sentence #20000, processed 129290 words and 137073 word types\n",
      "2017-04-29 20:59:27,974 : INFO : PROGRESS: at sentence #30000, processed 191151 words and 193141 word types\n",
      "2017-04-29 21:00:13,027 : INFO : PROGRESS: at sentence #40000, processed 255667 words and 250132 word types\n",
      "2017-04-29 21:00:53,997 : INFO : PROGRESS: at sentence #50000, processed 319987 words and 304641 word types\n",
      "2017-04-29 21:01:33,405 : INFO : PROGRESS: at sentence #60000, processed 383997 words and 357097 word types\n",
      "2017-04-29 21:02:12,294 : INFO : PROGRESS: at sentence #70000, processed 449297 words and 409551 word types\n",
      "2017-04-29 21:02:55,139 : INFO : PROGRESS: at sentence #80000, processed 513206 words and 459257 word types\n",
      "2017-04-29 21:03:35,740 : INFO : PROGRESS: at sentence #90000, processed 577067 words and 507968 word types\n",
      "2017-04-29 21:04:17,600 : INFO : PROGRESS: at sentence #100000, processed 641681 words and 556268 word types\n",
      "2017-04-29 21:04:52,671 : INFO : collected 599155 word types from a corpus of 699034 words (unigram + bigrams) and 109040 sentences\n",
      "2017-04-29 21:04:52,672 : INFO : using 599155 counts as vocab in Phrases<0 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000>\n",
      "2017-04-29 21:04:52,681 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2017-04-29 21:05:37,881 : INFO : adding document #10000 to Dictionary(17257 unique tokens: [u'ago\\u2026', u'cobij', u'recoj', u'caser_tratamient', u'argenpython']...)\n",
      "2017-04-29 21:06:16,485 : INFO : adding document #20000 to Dictionary(26011 unique tokens: [u'playstor', u'gauchit', u'spiderm', u'lacocinadlmied', u'cuidemosalplanet']...)\n",
      "2017-04-29 21:07:00,075 : INFO : adding document #30000 to Dictionary(32755 unique tokens: [u'playstor', u'gauchit', u'spiderm', u'tecnogek', u'lacocinadlmied']...)\n",
      "2017-04-29 21:07:44,286 : INFO : adding document #40000 to Dictionary(38863 unique tokens: [u'playstor', u'gauchit', u'spiderm', u'tecnogek', u'lacocinadlmied']...)\n",
      "2017-04-29 21:08:29,745 : INFO : adding document #50000 to Dictionary(44318 unique tokens: [u'playstor', u'gauchit', u'spiderm', u'tecnogek', u'lacocinadlmied']...)\n",
      "2017-04-29 21:09:12,263 : INFO : adding document #60000 to Dictionary(49224 unique tokens: [u'nachnop_mjoliv', u'playstor', u'gauchit', u'spiderm', u'tecnogek']...)\n",
      "2017-04-29 21:09:54,543 : INFO : adding document #70000 to Dictionary(54116 unique tokens: [u'moreauleopold', u'nachnop_mjoliv', u'playstor', u'gauchit', u'spiderm']...)\n",
      "2017-04-29 21:10:36,060 : INFO : adding document #80000 to Dictionary(58519 unique tokens: [u'moreauleopold', u'nachnop_mjoliv', u'playstor', u'gauchit', u'spiderm']...)\n",
      "2017-04-29 21:11:19,719 : INFO : adding document #90000 to Dictionary(62328 unique tokens: [u'moreauleopold', u'nachnop_mjoliv', u'playstor', u'gauchit', u'spiderm']...)\n",
      "2017-04-29 21:12:01,187 : INFO : adding document #100000 to Dictionary(66137 unique tokens: [u'moreauleopold', u'nachnop_mjoliv', u'playstor', u'gauchit', u'spiderm']...)\n",
      "2017-04-29 21:12:38,349 : INFO : built Dictionary(69541 unique tokens: [u'moreauleopold', u'nachnop_mjoliv', u'playstor', u'gauchit', u'spiderm']...) from 109040 documents (total 806892 corpus positions)\n",
      "2017-04-29 21:12:38,787 : INFO : discarding 43340 tokens: [(u'ferroc', 1), (u'bolognes', 1), (u'dieguez', 2), (u'mejorenbici', 1), (u'albahac', 2), (u'oregan', 1), (u'ciboulett', 1), (u'mejorenbicib', 1), (u'fabrici', 2), (u'fatrici', 1)]...\n",
      "2017-04-29 21:12:38,792 : INFO : keeping 26201 tokens which were in no less than 3 and no more than 32712 (=30.0%) documents\n",
      "2017-04-29 21:12:38,954 : INFO : resulting dictionary: Dictionary(26201 unique tokens: [u'moreauleopold', u'nachnop_mjoliv', u'lacocinadlmied', u'insolit', u'acusticazonacional']...)\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(trigram[get_docs(corpus)])\n",
    "dictionary.filter_extremes(no_below=3, no_above=0.3, keep_n=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'carohad', 26200),\n",
       " (u'pr_prolejsoy', 26199),\n",
       " (u'moratori_fiscal', 26198),\n",
       " (u'cifr_desaparec', 26197),\n",
       " (u'linchamient', 26196),\n",
       " (u'tan', 26195),\n",
       " (u'junt', 26194),\n",
       " (u'sab_abril', 26193),\n",
       " (u'aceleracion', 26192),\n",
       " (u'monserrat', 26191),\n",
       " (u'pi\\u2026', 26190),\n",
       " (u'lanch', 26189),\n",
       " (u'tecnopolisfederal_mision', 26188),\n",
       " (u'juni', 26187),\n",
       " (u'agu_contamin', 26186),\n",
       " (u'preguntasfrecuent', 26185),\n",
       " (u'casacreativadelsur', 26184),\n",
       " (u'emerg', 26183),\n",
       " (u'arame', 26182),\n",
       " (u'accionnotici', 26181),\n",
       " (u'jefatur_gabinet', 26180),\n",
       " (u'cupon', 26179),\n",
       " (u'consecuencias\\u201d', 26178),\n",
       " (u'radi_univers', 26177),\n",
       " (u'convencion', 26176),\n",
       " (u'buffy', 26175),\n",
       " (u'millone\\u2026', 26174),\n",
       " (u'zuranog', 26173),\n",
       " (u'viernessant', 26172),\n",
       " (u'abri_convocatori', 26171),\n",
       " (u'adriandelu', 26170),\n",
       " (u'automatizacion', 26169),\n",
       " (u'gveriti_adn', 26168),\n",
       " (u'nqnxev', 26167),\n",
       " (u'dema\\u2026', 26166),\n",
       " (u'convocatori_program', 26165),\n",
       " (u'anacastellani', 26164),\n",
       " (u'corporacion', 26163),\n",
       " (u'fundi', 26162),\n",
       " (u'periodismofederal_cn', 26161),\n",
       " (u'urquiz', 26160),\n",
       " (u'contradictori', 26159),\n",
       " (u'pataques', 26158),\n",
       " (u'tom_not', 26157),\n",
       " (u'ed', 26156),\n",
       " (u'torpez', 26155),\n",
       " (u'ciclist', 26154),\n",
       " (u'adrianaetelesur', 26153),\n",
       " (u'brusc', 26152),\n",
       " (u'jdiazok', 26151)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(dictionary.token2id.items(), key=lambda x:-x[1])[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-29 21:18:18,118 : INFO : saving Dictionary object under tweets_es.dict, separately None\n",
      "2017-04-29 21:18:18,160 : INFO : saved tweets_es.dict\n"
     ]
    }
   ],
   "source": [
    "dictionary.save(\"tweets_es.dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bow = [dictionary.doc2bow(doc) for doc in trigram[get_docs(corpus)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('tweets_es_bow.pickle', 'wb') as f:\n",
    "    pickle.dump(bow,f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
